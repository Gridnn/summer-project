{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "current page generator.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OXq1uv9ldRH"
      },
      "source": [
        "# Requirments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y19TLdihlG6T"
      },
      "source": [
        "!pip install regex==2019.4.14\n",
        "!pip install spacy==2.1.3\n",
        "!pip install editdistance==0.5.3\n",
        "!pip install requests==2.22.0\n",
        "!pip install StringBuilder==1.0.0a4\n",
        "!pip install clint==0.5.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQdpeUEmlpun"
      },
      "source": [
        "# Download the dumps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKPLcQ3WltuN"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "from clint.textui import progress\n",
        "def download_file(url):\n",
        "    local_filename = url.split('/')[-1]\n",
        "    response = requests.get(url, stream=True)\n",
        "    with open(local_filename, 'wb') as f:\n",
        "        total_length = int(response.headers.get('content-length'))\n",
        "        for chunk in progress.bar(response.iter_content(chunk_size=512),\n",
        "                                  expected_size=(total_length/512) + 1):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "    return local_filename"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upp8S1KllyYZ",
        "outputId": "518211ed-c867-48c9-dc65-2e86950a6e08"
      },
      "source": [
        "# for ZH:\n",
        "#infile = 'https://dumps.wikimedia.org/zhwiki/20210601/zhwiki-20210601-pages-meta-current.xml.bz2'\n",
        "# for BG:\n",
        "infile = 'https://dumps.wikimedia.org/bgwiki/20210601/bgwiki-20210601-pages-meta-current.xml.bz2'\n",
        "\n",
        "\n",
        "\n",
        "infile = infile.rstrip()\n",
        "fname = os.path.basename(infile) # get the basename of urls (the link after https://dumps.wikimedia.org/)\n",
        "if not os.path.exists(fname):\n",
        "    print(\"Downloading file : \" + fname)\n",
        "    download_file(infile)\n",
        "    print(\"Download finished!\")\n",
        "else:\n",
        "    print(\"File \" + fname + \" already downloaded!\")\n",
        "print(\"Filtering...\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading file : bgwiki-20210601-pages-meta-current.xml.bz2\n",
            "Download finished!\n",
            "Filtering...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI2nAFBcmuyG"
      },
      "source": [
        "# Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwK2zxJoqrIu"
      },
      "source": [
        "Define the functions for extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egF3I5jAmx_v"
      },
      "source": [
        "import logging\n",
        "import io\n",
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "import os\n",
        "import bz2\n",
        "from StringBuilder import StringBuilder\n",
        "\n",
        "class POVProcessor(object):\n",
        "    \"\"\"\n",
        "    Process a full revision history for a wikipedia page from the database dump and extract the revision tagged with\n",
        "    the list found in the tag file.\n",
        "    \"\"\"\n",
        "\n",
        "    default_tags = 'NPOV\\nEditorial'\n",
        "\n",
        "    def __init__(self, enc: str, output_file: str, logfile: str):\n",
        "        # if os.path.exists(os.path.join(os.getcwd(), tags_file)):\n",
        "        #     with open(os.path.join(os.getcwd(), tags_file), encoding=enc) as f:\n",
        "        #         tags = [re.escape(tag) for tag in f.read().strip().split('\\n')]\n",
        "        # else:\n",
        "        #     print(f'{tags_file} not found, using default tags' )\n",
        "        #     tags = [re.escape(tag) for tag in POVProcessor.default_tags.split('\\n')]\n",
        "        # self.regex = r'(?i){{((' + r'|'.join(tags) + r'))(\\|[^}]+)?}}' # FIXME: why ((..))\n",
        "        # self.match = lambda x: re.search(self.regex, str(x.find('text').text))\n",
        "\n",
        "        self.output = open(output_file, 'w', encoding=enc)\n",
        "        self.output_tags = open(output_file + '.alltags.txt', 'w', encoding=enc)\n",
        "        self.enc = enc\n",
        "        self.logit = False\n",
        "        if logfile:\n",
        "            logging.basicConfig(level=logging.INFO, handlers=[logging.FileHandler(logfile, 'w+', 'utf-8')])\n",
        "            self.logit = True\n",
        "\n",
        "    def normalize_ws(self, text: str):\n",
        "        return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    def extract(self, page: str):\n",
        "        if self.logit:\n",
        "            logging.info(\"Page : \" + page[page.find(\"<title>\") + 7:page.find(\"</title>\")])\n",
        "        article = None\n",
        "        id = None\n",
        "\n",
        "        # Look for all tags\n",
        "        # for x in re.finditer(self.all_tags, page):\n",
        "        #     pos = page.find('|', x.span()[0]+2, x.span()[1] - x.span()[0] - 4)\n",
        "        #     tg = page[x.span()[0]+2:x.span()[0]+2 + max(pos, min(self.tag_size_limit, x.span()[1] - x.span()[0] - 4))]\n",
        "        #     tg = tg.lower().strip() .replace('\\n', ' ')\n",
        "        #     if tg not in self.tags:\n",
        "        #         self.tags[tg] = 0\n",
        "        #     self.tags[tg] = self.tags[tg] + 1\n",
        "\n",
        "        # Based on http://effbot.org/elementtree/iterparse.htm\n",
        "        context = ET.iterparse(io.StringIO(page), events=(\"start\", \"end\"))\n",
        "        context = iter(context)\n",
        "        _, root = context.__next__()\n",
        "\n",
        "        for event, elem in context:\n",
        "            if event == 'end':\n",
        "                if elem.tag == 'page' and elem.find('id').text:\n",
        "                        id = elem.find('id').text\n",
        "                        article = self.normalize_ws(str(elem.find('revision').find('text').text)) or ''\n",
        "                        self.output.write(id + '\\t' + article.replace('\\t', ' ').replace('\\n', ' ') + '\\n')\n",
        "                        root.clear()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vYyoAqXqn8G"
      },
      "source": [
        "set the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCWOY71unlwi"
      },
      "source": [
        "enc = \"UTF-8\"\n",
        "report_freq = 1000\n",
        "lang = 'en'\n",
        "outputfile = 'revisions1.txt'\n",
        "pov = POVProcessor(enc, outputfile, fname + \".log\")\n",
        "cptPage = 0\n",
        "zip = bz2.BZ2File(fname)\n",
        "store = False\n",
        "fullpage = StringBuilder()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bmgafeipqi4M"
      },
      "source": [
        "extract all current Wikipedia pages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMOgeayooRkI",
        "outputId": "91a97c6d-33e5-49d0-90d0-56dc7aee081f"
      },
      "source": [
        "for line in zip:\n",
        "    line = line.decode(enc)\n",
        "\n",
        "    if store:\n",
        "        fullpage.append(line)\n",
        "        if line == \"  </page>\\n\":\n",
        "            \n",
        "            # Process it\n",
        "            \n",
        "            pov.extract(fullpage.to_string())\n",
        "            fullpage = StringBuilder()\n",
        "            store = False\n",
        "            cptPage += 1\n",
        "            if cptPage % report_freq == 0:\n",
        "                print(str(cptPage) + \" pages processed...\")\n",
        "\n",
        "        elif line.startswith(\"    <ns>\") and not line == \"    <ns>0</ns>\\n\":\n",
        "            # Other types of pages\n",
        "            fullpage = StringBuilder()\n",
        "            store = False\n",
        "\n",
        "    elif line == \"  <page>\\n\":\n",
        "        store = True\n",
        "        fullpage = StringBuilder()\n",
        "        fullpage.append(line)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000 pages processed...\n",
            "2000 pages processed...\n",
            "3000 pages processed...\n",
            "4000 pages processed...\n",
            "5000 pages processed...\n",
            "6000 pages processed...\n",
            "7000 pages processed...\n",
            "8000 pages processed...\n",
            "9000 pages processed...\n",
            "10000 pages processed...\n",
            "11000 pages processed...\n",
            "12000 pages processed...\n",
            "13000 pages processed...\n",
            "14000 pages processed...\n",
            "15000 pages processed...\n",
            "16000 pages processed...\n",
            "17000 pages processed...\n",
            "18000 pages processed...\n",
            "19000 pages processed...\n",
            "20000 pages processed...\n",
            "21000 pages processed...\n",
            "22000 pages processed...\n",
            "23000 pages processed...\n",
            "24000 pages processed...\n",
            "25000 pages processed...\n",
            "26000 pages processed...\n",
            "27000 pages processed...\n",
            "28000 pages processed...\n",
            "29000 pages processed...\n",
            "30000 pages processed...\n",
            "31000 pages processed...\n",
            "32000 pages processed...\n",
            "33000 pages processed...\n",
            "34000 pages processed...\n",
            "35000 pages processed...\n",
            "36000 pages processed...\n",
            "37000 pages processed...\n",
            "38000 pages processed...\n",
            "39000 pages processed...\n",
            "40000 pages processed...\n",
            "41000 pages processed...\n",
            "42000 pages processed...\n",
            "43000 pages processed...\n",
            "44000 pages processed...\n",
            "45000 pages processed...\n",
            "46000 pages processed...\n",
            "47000 pages processed...\n",
            "48000 pages processed...\n",
            "49000 pages processed...\n",
            "50000 pages processed...\n",
            "51000 pages processed...\n",
            "52000 pages processed...\n",
            "53000 pages processed...\n",
            "54000 pages processed...\n",
            "55000 pages processed...\n",
            "56000 pages processed...\n",
            "57000 pages processed...\n",
            "58000 pages processed...\n",
            "59000 pages processed...\n",
            "60000 pages processed...\n",
            "61000 pages processed...\n",
            "62000 pages processed...\n",
            "63000 pages processed...\n",
            "64000 pages processed...\n",
            "65000 pages processed...\n",
            "66000 pages processed...\n",
            "67000 pages processed...\n",
            "68000 pages processed...\n",
            "69000 pages processed...\n",
            "70000 pages processed...\n",
            "71000 pages processed...\n",
            "72000 pages processed...\n",
            "73000 pages processed...\n",
            "74000 pages processed...\n",
            "75000 pages processed...\n",
            "76000 pages processed...\n",
            "77000 pages processed...\n",
            "78000 pages processed...\n",
            "79000 pages processed...\n",
            "80000 pages processed...\n",
            "81000 pages processed...\n",
            "82000 pages processed...\n",
            "83000 pages processed...\n",
            "84000 pages processed...\n",
            "85000 pages processed...\n",
            "86000 pages processed...\n",
            "87000 pages processed...\n",
            "88000 pages processed...\n",
            "89000 pages processed...\n",
            "90000 pages processed...\n",
            "91000 pages processed...\n",
            "92000 pages processed...\n",
            "93000 pages processed...\n",
            "94000 pages processed...\n",
            "95000 pages processed...\n",
            "96000 pages processed...\n",
            "97000 pages processed...\n",
            "98000 pages processed...\n",
            "99000 pages processed...\n",
            "100000 pages processed...\n",
            "101000 pages processed...\n",
            "102000 pages processed...\n",
            "103000 pages processed...\n",
            "104000 pages processed...\n",
            "105000 pages processed...\n",
            "106000 pages processed...\n",
            "107000 pages processed...\n",
            "108000 pages processed...\n",
            "109000 pages processed...\n",
            "110000 pages processed...\n",
            "111000 pages processed...\n",
            "112000 pages processed...\n",
            "113000 pages processed...\n",
            "114000 pages processed...\n",
            "115000 pages processed...\n",
            "116000 pages processed...\n",
            "117000 pages processed...\n",
            "118000 pages processed...\n",
            "119000 pages processed...\n",
            "120000 pages processed...\n",
            "121000 pages processed...\n",
            "122000 pages processed...\n",
            "123000 pages processed...\n",
            "124000 pages processed...\n",
            "125000 pages processed...\n",
            "126000 pages processed...\n",
            "127000 pages processed...\n",
            "128000 pages processed...\n",
            "129000 pages processed...\n",
            "130000 pages processed...\n",
            "131000 pages processed...\n",
            "132000 pages processed...\n",
            "133000 pages processed...\n",
            "134000 pages processed...\n",
            "135000 pages processed...\n",
            "136000 pages processed...\n",
            "137000 pages processed...\n",
            "138000 pages processed...\n",
            "139000 pages processed...\n",
            "140000 pages processed...\n",
            "141000 pages processed...\n",
            "142000 pages processed...\n",
            "143000 pages processed...\n",
            "144000 pages processed...\n",
            "145000 pages processed...\n",
            "146000 pages processed...\n",
            "147000 pages processed...\n",
            "148000 pages processed...\n",
            "149000 pages processed...\n",
            "150000 pages processed...\n",
            "151000 pages processed...\n",
            "152000 pages processed...\n",
            "153000 pages processed...\n",
            "154000 pages processed...\n",
            "155000 pages processed...\n",
            "156000 pages processed...\n",
            "157000 pages processed...\n",
            "158000 pages processed...\n",
            "159000 pages processed...\n",
            "160000 pages processed...\n",
            "161000 pages processed...\n",
            "162000 pages processed...\n",
            "163000 pages processed...\n",
            "164000 pages processed...\n",
            "165000 pages processed...\n",
            "166000 pages processed...\n",
            "167000 pages processed...\n",
            "168000 pages processed...\n",
            "169000 pages processed...\n",
            "170000 pages processed...\n",
            "171000 pages processed...\n",
            "172000 pages processed...\n",
            "173000 pages processed...\n",
            "174000 pages processed...\n",
            "175000 pages processed...\n",
            "176000 pages processed...\n",
            "177000 pages processed...\n",
            "178000 pages processed...\n",
            "179000 pages processed...\n",
            "180000 pages processed...\n",
            "181000 pages processed...\n",
            "182000 pages processed...\n",
            "183000 pages processed...\n",
            "184000 pages processed...\n",
            "185000 pages processed...\n",
            "186000 pages processed...\n",
            "187000 pages processed...\n",
            "188000 pages processed...\n",
            "189000 pages processed...\n",
            "190000 pages processed...\n",
            "191000 pages processed...\n",
            "192000 pages processed...\n",
            "193000 pages processed...\n",
            "194000 pages processed...\n",
            "195000 pages processed...\n",
            "196000 pages processed...\n",
            "197000 pages processed...\n",
            "198000 pages processed...\n",
            "199000 pages processed...\n",
            "200000 pages processed...\n",
            "201000 pages processed...\n",
            "202000 pages processed...\n",
            "203000 pages processed...\n",
            "204000 pages processed...\n",
            "205000 pages processed...\n",
            "206000 pages processed...\n",
            "207000 pages processed...\n",
            "208000 pages processed...\n",
            "209000 pages processed...\n",
            "210000 pages processed...\n",
            "211000 pages processed...\n",
            "212000 pages processed...\n",
            "213000 pages processed...\n",
            "214000 pages processed...\n",
            "215000 pages processed...\n",
            "216000 pages processed...\n",
            "217000 pages processed...\n",
            "218000 pages processed...\n",
            "219000 pages processed...\n",
            "220000 pages processed...\n",
            "221000 pages processed...\n",
            "222000 pages processed...\n",
            "223000 pages processed...\n",
            "224000 pages processed...\n",
            "225000 pages processed...\n",
            "226000 pages processed...\n",
            "227000 pages processed...\n",
            "228000 pages processed...\n",
            "229000 pages processed...\n",
            "230000 pages processed...\n",
            "231000 pages processed...\n",
            "232000 pages processed...\n",
            "233000 pages processed...\n",
            "234000 pages processed...\n",
            "235000 pages processed...\n",
            "236000 pages processed...\n",
            "237000 pages processed...\n",
            "238000 pages processed...\n",
            "239000 pages processed...\n",
            "240000 pages processed...\n",
            "241000 pages processed...\n",
            "242000 pages processed...\n",
            "243000 pages processed...\n",
            "244000 pages processed...\n",
            "245000 pages processed...\n",
            "246000 pages processed...\n",
            "247000 pages processed...\n",
            "248000 pages processed...\n",
            "249000 pages processed...\n",
            "250000 pages processed...\n",
            "251000 pages processed...\n",
            "252000 pages processed...\n",
            "253000 pages processed...\n",
            "254000 pages processed...\n",
            "255000 pages processed...\n",
            "256000 pages processed...\n",
            "257000 pages processed...\n",
            "258000 pages processed...\n",
            "259000 pages processed...\n",
            "260000 pages processed...\n",
            "261000 pages processed...\n",
            "262000 pages processed...\n",
            "263000 pages processed...\n",
            "264000 pages processed...\n",
            "265000 pages processed...\n",
            "266000 pages processed...\n",
            "267000 pages processed...\n",
            "268000 pages processed...\n",
            "269000 pages processed...\n",
            "270000 pages processed...\n",
            "271000 pages processed...\n",
            "272000 pages processed...\n",
            "273000 pages processed...\n",
            "274000 pages processed...\n",
            "275000 pages processed...\n",
            "276000 pages processed...\n",
            "277000 pages processed...\n",
            "278000 pages processed...\n",
            "279000 pages processed...\n",
            "280000 pages processed...\n",
            "281000 pages processed...\n",
            "282000 pages processed...\n",
            "283000 pages processed...\n",
            "284000 pages processed...\n",
            "285000 pages processed...\n",
            "286000 pages processed...\n",
            "287000 pages processed...\n",
            "288000 pages processed...\n",
            "289000 pages processed...\n",
            "290000 pages processed...\n",
            "291000 pages processed...\n",
            "292000 pages processed...\n",
            "293000 pages processed...\n",
            "294000 pages processed...\n",
            "295000 pages processed...\n",
            "296000 pages processed...\n",
            "297000 pages processed...\n",
            "298000 pages processed...\n",
            "299000 pages processed...\n",
            "300000 pages processed...\n",
            "301000 pages processed...\n",
            "302000 pages processed...\n",
            "303000 pages processed...\n",
            "304000 pages processed...\n",
            "305000 pages processed...\n",
            "306000 pages processed...\n",
            "307000 pages processed...\n",
            "308000 pages processed...\n",
            "309000 pages processed...\n",
            "310000 pages processed...\n",
            "311000 pages processed...\n",
            "312000 pages processed...\n",
            "313000 pages processed...\n",
            "314000 pages processed...\n",
            "315000 pages processed...\n",
            "316000 pages processed...\n",
            "317000 pages processed...\n",
            "318000 pages processed...\n",
            "319000 pages processed...\n",
            "320000 pages processed...\n",
            "321000 pages processed...\n",
            "322000 pages processed...\n",
            "323000 pages processed...\n",
            "324000 pages processed...\n",
            "325000 pages processed...\n",
            "326000 pages processed...\n",
            "327000 pages processed...\n",
            "328000 pages processed...\n",
            "329000 pages processed...\n",
            "330000 pages processed...\n",
            "331000 pages processed...\n",
            "332000 pages processed...\n",
            "333000 pages processed...\n",
            "334000 pages processed...\n",
            "335000 pages processed...\n",
            "336000 pages processed...\n",
            "337000 pages processed...\n",
            "338000 pages processed...\n",
            "339000 pages processed...\n",
            "340000 pages processed...\n",
            "341000 pages processed...\n",
            "342000 pages processed...\n",
            "343000 pages processed...\n",
            "344000 pages processed...\n",
            "345000 pages processed...\n",
            "346000 pages processed...\n",
            "347000 pages processed...\n",
            "348000 pages processed...\n",
            "349000 pages processed...\n",
            "350000 pages processed...\n",
            "351000 pages processed...\n",
            "352000 pages processed...\n",
            "353000 pages processed...\n",
            "354000 pages processed...\n",
            "355000 pages processed...\n",
            "356000 pages processed...\n",
            "357000 pages processed...\n",
            "358000 pages processed...\n",
            "359000 pages processed...\n",
            "360000 pages processed...\n",
            "361000 pages processed...\n",
            "362000 pages processed...\n",
            "363000 pages processed...\n",
            "364000 pages processed...\n",
            "365000 pages processed...\n",
            "366000 pages processed...\n",
            "367000 pages processed...\n",
            "368000 pages processed...\n",
            "369000 pages processed...\n",
            "370000 pages processed...\n",
            "371000 pages processed...\n",
            "372000 pages processed...\n",
            "373000 pages processed...\n",
            "374000 pages processed...\n",
            "375000 pages processed...\n",
            "376000 pages processed...\n",
            "377000 pages processed...\n",
            "378000 pages processed...\n",
            "379000 pages processed...\n",
            "380000 pages processed...\n",
            "381000 pages processed...\n",
            "382000 pages processed...\n",
            "383000 pages processed...\n",
            "384000 pages processed...\n",
            "385000 pages processed...\n",
            "386000 pages processed...\n",
            "387000 pages processed...\n",
            "388000 pages processed...\n",
            "389000 pages processed...\n",
            "390000 pages processed...\n",
            "391000 pages processed...\n",
            "392000 pages processed...\n",
            "393000 pages processed...\n",
            "394000 pages processed...\n",
            "395000 pages processed...\n",
            "396000 pages processed...\n",
            "397000 pages processed...\n",
            "398000 pages processed...\n",
            "399000 pages processed...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcam6fQOo2ws"
      },
      "source": [
        "# Sentencizer and pre-process the articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOLgIbh-qws3"
      },
      "source": [
        "Define the functions for sentencizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3mbkVFHpIFc"
      },
      "source": [
        "import spacy\n",
        "import argparse\n",
        "\n",
        "if lang is not 'zh':\n",
        "    nlp = spacy.blank(lang)\n",
        "    nlp.max_length = 2000000\n",
        "    sentencizer = nlp.create_pipe(\"sentencizer\")\n",
        "    nlp.add_pipe(sentencizer)\n",
        "\n",
        "\n",
        "def sbd(string):\n",
        "    \"\"\"\n",
        "    Takes a string and returns a list of sentences\n",
        "    longer than 5 tokens.\n",
        "    string: param\n",
        "    \"\"\"\n",
        "    if lang == 'zh':\n",
        "        sentences = re.split('(。|！|\\!|\\.|？|\\?)',string)\n",
        "        return [sent for sent in sentences if 10 < len(sent) < 300]\n",
        "    else:\n",
        "        doc = nlp(string)\n",
        "        doc.is_parsed = True\n",
        "        return [str(sent) for sent in list(doc.sents) if 5 < len(sent) < 300]"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHaIz7i_q3Hu"
      },
      "source": [
        "Define the functions for pre-preprocessing (data cleanning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO1DUbLSqGA0"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import argparse\n",
        "# from utils import check_lang\n",
        "def clean_wiki(text):\n",
        "    text = re.sub(r\"''+\", '', text)  # wiki bold and double quotes\n",
        "    text = re.sub(r'==+.+?==+', '', text)  # wiki titles\n",
        "    text = re.sub(r'<ref[^<]*<\\/ref>', '', text)  # remove references <ref...> ... </ref>\n",
        "    text = re.sub(r'<!--[\\s\\S\\n]*?-->', '', text)  # html comments\n",
        "    text = re.sub(r'<([A-Z][A-Z0-9]*)\\b[^>]*>(.*?)</\\1>', '', text)  # <tag>...</tag>\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'<[^>|<]*/>', '', text)  # self-closing html tags\n",
        "    text = re.sub(r'\\[http:[^] ]*', '[', text)  # remove normal url, preserve visible text\n",
        "    text = re.sub(r'\\[\\[[^\\|\\]]*\\|', '[[', text)  # remove wiki url, preserve visible text\n",
        "    text = re.sub(r'\\[\\[[^\\]]*:[^\\]|\\|]*\\]\\]', '', text)  # remove links to other languages & categories\n",
        "    text = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)', '', text)  # remove bare URLs\n",
        "    text = regex.sub(r'\\{\\{((?>[^{}]+|(?R))*)\\}\\}', '', text)  # remove embedded {{icons and comments}} recursively\n",
        "    text = re.sub(r'\\{[^\\}]*\\}', '', text)  # remove {tables}\n",
        "    text = re.sub(r'\\[[^\\[]*\\|[^\\]]*', '', text)  # remove [image|titles|& other tags]\n",
        "    text = re.sub(r'\\}|\\{', '', text)  # trailing {}\n",
        "    text = re.sub(r'\\]|\\[', '', text)  # trailing []\n",
        "    text = re.sub(r'<[^>]*>', '', text)  # trailing HTML tags\n",
        "    text = re.sub(r'&.{4,6};', '', text)  # HTML unicode characters\n",
        "    text = re.sub(r'\\s\\*', '.', text)  # end list items with . and remove bullet points\n",
        "    if lang.lower() == \"bg\":\n",
        "        text = re.sub(r'\\d+((,|\\.)*\\d+)*', 'НУМТКН ', text)  # replace numbers by a token in Cyrillic\n",
        "        text = re.sub(r'[a-zA-Z]', '', text)  # remove latin script\n",
        "    text = re.sub(r'\\d+((,|\\.)*\\d+)*', 'NUMTKN ', text)  # replace numbers by a token\n",
        "    text = re.sub(r'[. ]{3,}', '. ', text)  # clean multiple .\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)  # clean multiple spaces\n",
        "    text = re.sub(r'!{10,}', '', text)  # remove excessive exclamation points\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_punct(text):\n",
        "    if lang.lower() == \"fr\":\n",
        "        text = re.sub(r\"[.,\\/#!?†‡•$%‰\\^&\\*;:{}|=–\\-_—‗‾⁄`~′″‴‵‶‷()‚‛“”„‟‹›№«»]\", ' ', text) # keep U+0027, U+2018 and U+2019 (=apostrophies)\n",
        "    elif lang.lower() == \"zh\":\n",
        "        punc = \"！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃《》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\"\n",
        "        text = re.sub(r\"[%s]+\" %punc, ' ', text)\n",
        "    #else:\n",
        "    text = re.sub(r\"[.,\\/#!?†‡•$%‰\\^&\\*;:{}|=–\\-_—‗‾⁄`~′″‴‵‶‷()‘’‚‛“”„‟‹›'№]\", ' ', text)  # remove punctuation and symbols\n",
        "    text = re.sub(r'\"', ' ', text)\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)  # clean multiple spaces\n",
        "    return text.strip()\n",
        "\n",
        "def clean_no_chinese(text):\n",
        "      chinese_term = re.findall(r'[\\u4e00-\\u9fff]+', text)    \n",
        "      if chinese_term == []:\n",
        "          return ''\n",
        "      elif len(''.join(chinese_term)) <= 10:\n",
        "          return ''\n",
        "      return text"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78O6L-6irD-U"
      },
      "source": [
        "Define the pipeline for pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17ZP3rLqqNjD"
      },
      "source": [
        "def preprocess(revision):\n",
        "    rev = dict()\n",
        "    try:\n",
        "        rev['id'], rev['text'] = revision.strip().split('\\t')\n",
        "        return rev\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "def clean_text(dico):\n",
        "    rev = dict()\n",
        "    rev['id'] = dico['id']\n",
        "    rev['text'] = [sent for sent in sbd(clean_wiki(dico['text']))]\n",
        "    return rev\n",
        "\n",
        "def punct_diff(dico):\n",
        "    rev = dict()\n",
        "    rev['id'] = dico['id']\n",
        "    # further process the before and after state of the revision\n",
        "    rev['text'] = [clean_punct(sent).lower() for sent in dico['text']]\n",
        "    # remove too short or too long sentences\n",
        "    rev['text'] = filter_length(rev['text'])\n",
        "    # cross check to get updated lists of rem/add sentences\n",
        "    # remove the text if it does not contains Chinese\n",
        "    if lang.lower() == 'zh':\n",
        "      rev['text'] = [clean_no_chinese(sent) for sent in rev['text']]\n",
        "    return rev\n",
        "\n",
        "def filter_length(sents):\n",
        "    return [sent for sent in sents if 10 < len(sent) < 300]\n",
        "\n",
        "def process(input_file):   \n",
        "    with open(input_file, \"r\") as file:\n",
        "        for line in file:\n",
        "            if preprocess(line) is None:\n",
        "                stats['ignored, short'] += 1\n",
        "                continue\n",
        "            revs = preprocess(line)\n",
        "            revs = clean_text(revs)\n",
        "            revs = punct_diff(revs)\n",
        "            yield revs"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HkllrC2qUqv"
      },
      "source": [
        "inputfile = 'revisions1.txt'\n",
        "#outputfile = 'diff.to_pickle'\n",
        "data = [d for d in process(inputfile)]\n",
        "#to_pickle(data, diffs.pickle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu-Lnd4Orv4a"
      },
      "source": [
        "# Write the data to txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMIZU_yWstTN"
      },
      "source": [
        "def to_text_file(data, output_file_path):\n",
        "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write('\\n'.join(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw9YfGO0rzyy"
      },
      "source": [
        "sents = []\n",
        "for i in range(len(data)):\n",
        "  for sent in data[i]['text']:\n",
        "    if len(sent) >= 5:\n",
        "      sents.append(sent + '\\t' + data[i]['id'])\n",
        "\n",
        "to_text_file(sents, lang.upper() + '.txt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}